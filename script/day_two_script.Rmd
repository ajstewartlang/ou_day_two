---
title: "Day Two"
author: "[Andrew Stewart](https://ajstewartlang.netlify.app/)"
date: "10th July 2021"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

# Simple Linear Regression
We're going to start off by looking at simple linear regression. This is when we have one predictor and one outcome variable. Remember to create a new `.Rproj` file to keep things organised. Once you've created that, then you can start with a new script.

## The Packages We Need

First we need to install the packages we need.  We're going to install the `tidyverse` packages plus a few others. The package `Hmisc` allows us to use the `rcorr()` function for calculating Pearson's r, and the `performance` package so we can test our model assumptions.  Remember, if you haven't previously installed these packages on your laptop you first need to type `install.packages("packagename")` in the console before you can call the `library()` function for that package. You *may* also need to install the package `see` to get the `performance` package working. If so, do that in the console by typing `install.packages("see")`.

```{r, message=FALSE}
library(tidyverse)
library(Hmisc)
library(performance)
```

## Import the Data

Import the dataset called `crime_dataset.csv` - this dataset contains population data, housing price index data and crime data for cities in the US. I've stored the data on my website so we can just load it from there.

We can use the function `head()` to display the first few rows of our dataset called "crime".

```{r, message=FALSE}
crime <- read_csv("https://raw.githubusercontent.com/ajstewartlang/09_glm_regression_pt1/master/data/crime_dataset.csv")
head(crime)
```

## Tidy the Data

First let's do some wrangling.  There is one column that combines both City and State information. Let's separate that information out into two new columns called "City" and "State" using the function `separate()`. Then have a look at what you now have. How has the output of `head(crime)` changed from above?

```{r}
crime <- separate(crime, col = "City, State", into = c("City", "State"))
head(crime)
```

Now let's rename the columns to change the name of the "index_nsa" column to "House_price" and get rid of the space in the "Violent Crimes" heading.  See how the output of `head(crime)` has changed again?

```{r}
crime <- crime %>%
  rename(House_price = index_nsa) %>%
  rename(Violent_Crimes = "Violent Crimes")
head(crime)
```

## Plot the Data

We might first think that as population size increases, crime rate also increases.  Let's first build a scatter plot.

```{r, warning=FALSE, message=FALSE}
crime %>%
  ggplot(aes(x = Population, y = Violent_Crimes)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  theme(text = element_text(size = 13)) +
  labs(x = "Population", 
       y = "Violent Crimes")
```

## Pearson's r

This plot looks pretty interesting.  How about calculating Pearson's r?

```{r}
rcorr(crime$Population, crime$Violent_Crimes)
```

Look at the r and p-values - r is =.81 and p < .001. So ~64% of the variance in our Violent_Crimes variable is explained by our Population size variable.  Clearly there is a positive relationship between population size and the rate of violent crime. From the plot, we might conclude that the relationship is being overly influenced by crime in a small number of very large cities (top right of the plot above).  Let's exclude cities with populations greater than 2,000,000

```{r}
crime_filtered <- filter(crime, Population < 2000000)
```

Now let's redo the plot.  As there are still likely to be quite a lot of points (and thus overplotting with many points appearing roughly in the same place), we can set the alpha parameter to be < 1 in the `geom_point()` line of code. This parameter corresponds to the translucency of each point. Change it to other values to see what happens. 

```{r, warning=FALSE, message=FALSE}
crime_filtered %>%
  ggplot(aes(x = Population, y = Violent_Crimes)) + 
  geom_point(alpha = .25) + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  theme(text = element_text(size = 13)) +
  labs(x = "Population", 
       y = "Violent Crimes")
```

And calculate Pearson's r.

```{r}
rcorr(crime_filtered$Population, crime_filtered$Violent_Crimes)
```

There is still a clear positive relationship (r=.69).  Let's build a linear model. The dataset contains a lot of data and each city appears a number of times (once each year). For our linear model, our observations need to be independent of each other so let's just focus on the year 2015. That way each city will just appear once.

First we apply our filter.

```{r}
crime_filtered <- filter(crime_filtered, Year == 2015)
```

Then we build a plot. I'm using the layer `geom_text()` to plot the City names and set the check_overlap parameter to `TRUE` to ensure the labels don't overlap.

```{r, warning=FALSE, message=FALSE}
crime_filtered %>%
  ggplot(aes(x = Population, y = Violent_Crimes, label = City)) + 
  geom_point() + 
  geom_text(nudge_y = 500, check_overlap = TRUE) + 
  geom_smooth(method = "lm", se = FALSE) + 
  xlim(0, 1800000) +
  theme_minimal() +
  theme(text = element_text(size = 13)) +
  labs(x = "Population", 
       y = "Violent Crimes")
```

This shows a clear positive linear relationship so let's work out Pearson's r.

```{r}
rcorr(crime_filtered$Population, crime_filtered$Violent_Crimes)
```

## Model the Data

Imagine we are a city planner, and we want to know by how much we think violent crimes might increase as a function of population size. In other words, we want to work out how the violent crime rate is predicted by population size.

We're going to build two linear models - one `model1` where we're using the mean of our outcome variable as the predictor, and a second `model2` where we are using Population size to predict the Violent Crimes outcome.

```{r}
model1 <- lm(Violent_Crimes ~ 1, data = crime_filtered)
model2 <- lm(Violent_Crimes ~ Population, data = crime_filtered)
```

## Checking Our Assumptions

Let's use the `check_model()` function from the performance package to check the assumptions of our model.

```{r, warning=FALSE, message=FALSE}
check_model(model2)
```

Our dataset is small and so some of our diagnostic plots don't look great. We'll come back to the influential outlier (case 29) later - but for now let's use the `anova()` function to see if our model with Population as the predictor is better than the one using just the mean.

```{r}
anova(model1, model2)
```

It is - the models differ and you'll see the residual sum of squares (or the error) is less in the second model (which has Population as the predictor). This means the deviation between our observed data and the regression line model `model2` is significantly less than the deviation between our observed data and the mean as a model of our data `model1`. So let's get the parameter estimates of `model2`.

## Interpreting Our Model

```{r}
summary(model2)
```

The intercept corresponds to where our regression line intercepts the y-axis, and the Population parameter corresponds to the slope of our line. We see that for every increase in population by 1 there is an extra 0.006963 increase in violent crime. 

For a city with a population of about a million, there will be about 7907 Violent Crimes. We calculate this by multiplying the estimate of our predictor (0.006963) by 1,000,000 and then adding the intercept (944.3).  This gives us 7907.3 crimes - which tallys with what you see in our regression line above. We may have a few outliers - how would you figure out what those were? Try excluding any outliers you find and re-building your model.

## Breakout Rooms

You now have three tasks:<br>
1. Check whether the same relationship holds for population size and robberies in 2015.<br>
2. Are house prices predicted by the number of violent crimes in 2015?<br>
3. Are house prices predicted by population size in 2015? 

# Multiple Regression

In standard multiple regression all the independent variables (IVs) are entered into the equation and evaluated for their contribution at the same time. Let’s work through a specific example.

An educational psychologist conducted a study that investigated the psycholinguistic variables that contribute to spelling performance in primary school children aged between 7- and 9-years. The researcher presented children with 48 words that varied systematically according to certain features such as age of acquisition, word frequency, word length, and imageability. The psychologist wants to check whether performance on the test accurately reflected children’s spelling ability as estimated by a standardised spelling test. That is, the psychologist wants to check whether her test was appropriate.

Children’s chronological age (in months) `(age)`, their reading age `(RA)`, their standardised reading age `(std_RA)`, and their standardised spelling score `(std_SPELL)` were chosen as predictor variables. The outcome variable was the percentage correct spelling `(corr_spell)` score attained by each child using the list of 48 words. 

First we need to load the packages we need - the require function assumes they are already on your machine. If they are not, then you need to `install.packages ("packagename")` first:

## The Packages We Need

In addition to the packages we used earlier, we also need the following: `{MASS}`, `{car}`, and `{olsrr}`.

```{r, message=FALSE}
library(MASS) # Needed for maths functions
library(car) # Needed for VIF calculation
```

## Import the Data

You now need to read in the data file from my website.

```{r, message=FALSE}
MRes_tut2 <- read_csv("https://raw.githubusercontent.com/ajstewartlang/10_glm_regression_pt2/master/data/MRes_tut2.csv")
```

### Examining Possible Relationships

Before we start, let's look at the relationships between our IVs (predictors) and our DV (outcome).  We can plot graphs depicting the correlations.  We'll plot test performance against each of our four predictors in turn:

```{r, message=FALSE}
ggplot(MRes_tut2, aes(x = age, y = corr_spell)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  theme(text = element_text(size = 13)) 

ggplot(MRes_tut2, aes(x = RA, y = corr_spell)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  theme(text = element_text(size = 13)) 

ggplot(MRes_tut2, aes(x = std_RA, y = corr_spell)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  theme(text = element_text(size = 13)) 

ggplot(MRes_tut2, aes(x = std_SPELL, y = corr_spell)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  theme(text = element_text(size = 13)) 
```

## Model the Data

We'll build one model (which we'll call `model0`) that is the mean of our outcome variable, and another model (`model1`) which contains all our predictors:

```{r}
model0 <- lm(corr_spell ~ 1, data = MRes_tut2)
model1 <- lm(corr_spell ~ age + RA + std_RA + std_SPELL, data = MRes_tut2)
```

Let's compare them to each other:

```{r}
anova(model0, model1)
```

We see that the models differ from each other (look a the *p*-value of the comparison) and that the model with the four predictors has the lower Residuals (RSS) value meaning there is less error between the model and the observed data relative to the simpler intercept-only model (i.e., the mean) and the observed data.

### Checking our Assumptions

OK, so they differ - now let's plot information about our model assumptions - remember, we are particularly interested in Cook's distance values for our case...

```{r, warning=FALSE, message=FALSE}
check_model(model1)
```

The errors looks fairly equally distributed along our fitted values (homoscedasticity) - although a little worse for high fitted values - and from the Q-Q plot we can tell they look fairly normal (they should follow the diagonal).  How about influential cases?  So, Case 10 looks a bit dodgy - it has a high Cook's Distance value - which suggests it is having a disproportionate effect on our model.  Let's exclude it using the `filter()` function - the symbol `!=` means 'does not equal' so we are selecting values other than Case 10.  

### Dropping an Influential Case

```{r}
MRes_tut2_drop10 <- filter(MRes_tut2, case != "10")
```

### Re(model) the Data

We now create another model (`model2`) which doesn't include Case 10.

```{r}
model2 <- lm(corr_spell ~ age + RA + std_RA + std_SPELL, data = MRes_tut2_drop10)
```

Let's check the model assumptions again using `check_model()`.

### Checking our Assumptions

```{r, warning=FALSE, message=FALSE}
check_model(model2)
```

Now, let's look at the multicollinearity values measured by VIF:

```{r}
vif(model2)
```

It looks like RA and std_RA are problematic.  We can look at the correlation between them using the `rcorr()` function:

```{r}
rcorr(MRes_tut2_drop10$RA, MRes_tut2_drop10$std_RA)
```

### Re(model) the Data

The correlation is pretty high (0.88), so let's exclude the predictor with the highest VIF value (which is RA) and build a new model:

```{r}
model3 <- lm(corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10)
vif(model3)
```

### Checking our Assumptions

These values look ok now. Let's check the model assumptions again.

```{r, warning=FALSE, message=FALSE}
check_model(model3)
```

### Summary of our Model

Now let's generate the coefficients as this looks like a sensible model.

```{r}
summary(model3)
model0 <- lm(corr_spell ~ 1, data = MRes_tut2_drop10)
anova(model3, model0)
```

We'd write our equation as something like:

`Spelled correct = -209.44 + 1.10(age) + 0.38(std_RA) + 1.21(std_SPELL) + residual`

# Stepwise regression

We can also do stepwise regression - forwards is when you start with the null model and predictors are added until they don't explain any more variance, backwards is when you start with the full model and remove predictors until removal starts affecting your model's predictive ability. Let's keep case 10 dropped and also drop the high VIF predictor (RA). This is handy for models with lots of predictors where the order in sequential regression is not obvious. 

## Model the Data

```{r}
model0 <- lm(corr_spell ~ 1, data = MRes_tut2_drop10)
model1 <- lm(corr_spell ~ age + std_RA + std_SPELL, data = MRes_tut2_drop10)
```

Let's do stepwise forwards:

```{r}
steplimitsf <- step(model0, scope = list (lower = model0, upper = model1), direction = "forward")
summary(steplimitsf)
```

Stepwise backwards:

```{r}
steplimitsb <- step(model1, direction = "back")
summary(steplimitsb)
```

And stepwise using both forwards and backwards procedures:

```{r}
steplimitsboth <- step(model0, scope = list (upper = model1), direction = "both")
```

### Checking our Assumptions

```{r, warning=FALSE, message=FALSE}
check_model(steplimitsboth)
```

These look ok.

```{r}
summary(steplimitsboth)
```

You'll see that the same final model is arrived it in each case. We have three significant predictors.
